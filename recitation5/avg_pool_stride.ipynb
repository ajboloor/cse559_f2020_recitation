{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = edf.ops\n",
    "params = edf.params\n",
    "values = edf.values\n",
    "\n",
    "# Average pooling with stride\n",
    "class avg_pool_with_stride:\n",
    "    def __init__(self,x,sz,stride):\n",
    "        ops.append(self)\n",
    "        self.x = x\n",
    "        self.sz = sz\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self):\n",
    "        B,H,W,C = self.x.top.shape\n",
    "        nH, nW = (H-self.sz)//self.stride+1, (W-self.sz)//self.stride+1\n",
    "        top = np.zeros([B,nH,nW,C])\n",
    "        for i in range(self.sz):\n",
    "            for j in range(self.sz):\n",
    "                xcrop = self.x.top[:, i:(H-self.sz+1+i):self.stride, j:(W-self.sz+1+j):self.stride, :].copy()\n",
    "                top = top + xcrop\n",
    "        \n",
    "        self.top = top / np.float32(self.sz*self.sz)\n",
    "#         print(\"\\nforward\")\n",
    "#         print(self.x.top[0,:,:,0])\n",
    "#         print(self.top[0,:,:,0])\n",
    "\n",
    "    def backward(self):\n",
    "        if self.x in ops or self.x in params:\n",
    "            B,H,W,C = self.x.top.shape\n",
    "            xgrad = np.zeros([B,H,W,C])\n",
    "            for i in range(self.sz):\n",
    "                for j in range(self.sz):\n",
    "                    xgrad[:, i:(H-self.sz+1+i):self.stride, j:(W-self.sz+1+j):self.stride, :] += self.grad / (self.sz*self.sz)\n",
    "\n",
    "            self.x.grad = self.x.grad + xgrad\n",
    "#             print(\"\\nbackward\")\n",
    "#             print(self.grad[0,:,:,0])\n",
    "#             print(self.x.grad[0,:,:,0])\n",
    "            \n",
    "            \n",
    "edf.avg_pool_with_stride = avg_pool_with_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "[[ 0  2  4  6]\n",
      " [ 8 10 12 14]\n",
      " [16 18 20 22]\n",
      " [24 26 28 30]]\n"
     ]
    }
   ],
   "source": [
    "# #######################################\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Param()\n",
    "lab = edf.Value()\n",
    "\n",
    "# Model\n",
    "y = edf.avg_pool_with_stride(inp,2,2)\n",
    "\n",
    "# Loss\n",
    "loss = edf.add(y,lab)\n",
    "\n",
    "# Forward test\n",
    "data = np.arange(32).reshape([1,4,4,2])\n",
    "\n",
    "print(\"data\")\n",
    "print(data[0, :, :, 0])\n",
    "\n",
    "inp.set(data)\n",
    "l = np.ones([1,2,2,2])*(-1.0)\n",
    "lab.set(l)\n",
    "\n",
    "edf.Forward()\n",
    "\n",
    "# Backward test\n",
    "edf.Backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000: #### 0 Epochs: Val Loss = 2.324e+00, Accuracy = 9.60%\n",
      "000000050: Training Loss = 2.311e+00, Accuracy = 10.28%\n",
      "000000100: Training Loss = 2.290e+00, Accuracy = 12.32%\n",
      "000000150: Training Loss = 2.273e+00, Accuracy = 16.68%\n",
      "000000200: Training Loss = 2.250e+00, Accuracy = 16.32%\n",
      "000000250: Training Loss = 2.218e+00, Accuracy = 25.24%\n",
      "000000300: Training Loss = 2.170e+00, Accuracy = 41.12%\n",
      "000000350: Training Loss = 2.088e+00, Accuracy = 47.16%\n",
      "000000400: Training Loss = 1.950e+00, Accuracy = 54.60%\n",
      "000000450: Training Loss = 1.706e+00, Accuracy = 65.20%\n",
      "000000500: Training Loss = 1.320e+00, Accuracy = 72.32%\n",
      "000000500: #### 1 Epochs: Val Loss = 1.104e+00, Accuracy = 75.80%\n",
      "000000550: Training Loss = 1.012e+00, Accuracy = 75.00%\n",
      "000000600: Training Loss = 8.006e-01, Accuracy = 79.24%\n",
      "000000650: Training Loss = 6.916e-01, Accuracy = 80.28%\n",
      "000000700: Training Loss = 5.818e-01, Accuracy = 82.36%\n",
      "000000750: Training Loss = 5.467e-01, Accuracy = 84.16%\n",
      "000000800: Training Loss = 5.198e-01, Accuracy = 84.36%\n",
      "000000850: Training Loss = 5.401e-01, Accuracy = 84.32%\n",
      "000000900: Training Loss = 4.881e-01, Accuracy = 85.36%\n",
      "000000950: Training Loss = 4.926e-01, Accuracy = 84.76%\n",
      "000001000: Training Loss = 4.459e-01, Accuracy = 87.00%\n",
      "000001000: #### 2 Epochs: Val Loss = 4.233e-01, Accuracy = 87.30%\n",
      "000001050: Training Loss = 4.363e-01, Accuracy = 87.56%\n",
      "000001100: Training Loss = 4.477e-01, Accuracy = 86.60%\n",
      "000001150: Training Loss = 4.485e-01, Accuracy = 86.04%\n",
      "000001200: Training Loss = 4.652e-01, Accuracy = 85.24%\n",
      "000001250: Training Loss = 4.475e-01, Accuracy = 85.12%\n",
      "000001300: Training Loss = 4.745e-01, Accuracy = 84.76%\n",
      "000001350: Training Loss = 4.514e-01, Accuracy = 85.84%\n",
      "000001400: Training Loss = 4.284e-01, Accuracy = 87.40%\n",
      "000001450: Training Loss = 4.152e-01, Accuracy = 87.12%\n",
      "000001500: Training Loss = 4.139e-01, Accuracy = 87.40%\n",
      "000001500: #### 3 Epochs: Val Loss = 3.676e-01, Accuracy = 89.20%\n",
      "000001550: Training Loss = 4.010e-01, Accuracy = 87.96%\n",
      "000001600: Training Loss = 4.175e-01, Accuracy = 86.64%\n",
      "000001650: Training Loss = 4.136e-01, Accuracy = 87.32%\n",
      "000001700: Training Loss = 4.298e-01, Accuracy = 86.84%\n",
      "000001750: Training Loss = 4.083e-01, Accuracy = 88.40%\n",
      "000001800: Training Loss = 3.838e-01, Accuracy = 88.32%\n",
      "000001850: Training Loss = 3.940e-01, Accuracy = 87.60%\n",
      "000001900: Training Loss = 4.193e-01, Accuracy = 87.12%\n",
      "000001950: Training Loss = 4.085e-01, Accuracy = 87.44%\n",
      "000002000: Training Loss = 4.104e-01, Accuracy = 87.88%\n",
      "000002000: #### 4 Epochs: Val Loss = 3.586e-01, Accuracy = 89.40%\n",
      "000002050: Training Loss = 3.291e-01, Accuracy = 89.88%\n",
      "000002100: Training Loss = 3.984e-01, Accuracy = 88.00%\n",
      "000002150: Training Loss = 4.088e-01, Accuracy = 87.16%\n",
      "000002200: Training Loss = 3.889e-01, Accuracy = 87.68%\n",
      "000002250: Training Loss = 3.864e-01, Accuracy = 88.28%\n",
      "000002300: Training Loss = 4.188e-01, Accuracy = 87.20%\n",
      "000002350: Training Loss = 3.919e-01, Accuracy = 88.44%\n",
      "000002400: Training Loss = 3.906e-01, Accuracy = 88.08%\n",
      "000002450: Training Loss = 3.816e-01, Accuracy = 88.04%\n",
      "000002500: Training Loss = 3.928e-01, Accuracy = 88.32%\n",
      "000002500: #### 5 Epochs: Val Loss = 3.411e-01, Accuracy = 89.70%\n",
      "000002550: Training Loss = 3.896e-01, Accuracy = 88.36%\n",
      "000002600: Training Loss = 3.785e-01, Accuracy = 88.28%\n",
      "000002650: Training Loss = 3.639e-01, Accuracy = 88.96%\n",
      "000002700: Training Loss = 3.698e-01, Accuracy = 89.08%\n",
      "000002750: Training Loss = 3.756e-01, Accuracy = 89.36%\n",
      "000002800: Training Loss = 3.515e-01, Accuracy = 88.92%\n",
      "000002850: Training Loss = 3.660e-01, Accuracy = 87.96%\n",
      "000002900: Training Loss = 3.518e-01, Accuracy = 89.44%\n",
      "000002950: Training Loss = 3.774e-01, Accuracy = 88.44%\n",
      "000003000: Training Loss = 3.680e-01, Accuracy = 89.16%\n",
      "000003000: #### 6 Epochs: Val Loss = 3.167e-01, Accuracy = 91.50%\n",
      "000003050: Training Loss = 3.496e-01, Accuracy = 89.36%\n",
      "000003100: Training Loss = 3.494e-01, Accuracy = 88.92%\n",
      "000003150: Training Loss = 3.365e-01, Accuracy = 89.68%\n",
      "000003200: Training Loss = 3.471e-01, Accuracy = 89.84%\n",
      "000003250: Training Loss = 3.708e-01, Accuracy = 88.80%\n",
      "000003300: Training Loss = 3.669e-01, Accuracy = 89.44%\n",
      "000003350: Training Loss = 3.590e-01, Accuracy = 89.64%\n",
      "000003400: Training Loss = 3.649e-01, Accuracy = 89.68%\n",
      "000003450: Training Loss = 3.396e-01, Accuracy = 89.80%\n",
      "000003500: Training Loss = 3.496e-01, Accuracy = 89.20%\n",
      "000003500: #### 7 Epochs: Val Loss = 3.132e-01, Accuracy = 90.80%\n",
      "000003550: Training Loss = 3.541e-01, Accuracy = 89.84%\n",
      "000003600: Training Loss = 3.316e-01, Accuracy = 89.92%\n",
      "000003650: Training Loss = 3.182e-01, Accuracy = 90.16%\n",
      "000003700: Training Loss = 3.108e-01, Accuracy = 90.64%\n",
      "000003750: Training Loss = 3.498e-01, Accuracy = 89.32%\n",
      "000003800: Training Loss = 3.622e-01, Accuracy = 89.00%\n",
      "000003850: Training Loss = 3.366e-01, Accuracy = 90.00%\n",
      "000003900: Training Loss = 3.266e-01, Accuracy = 90.16%\n",
      "000003950: Training Loss = 3.564e-01, Accuracy = 89.20%\n",
      "000004000: Training Loss = 3.425e-01, Accuracy = 89.84%\n",
      "000004000: #### 8 Epochs: Val Loss = 3.108e-01, Accuracy = 92.00%\n",
      "000004050: Training Loss = 3.176e-01, Accuracy = 90.44%\n",
      "000004100: Training Loss = 3.205e-01, Accuracy = 90.60%\n",
      "000004150: Training Loss = 3.162e-01, Accuracy = 90.52%\n",
      "000004200: Training Loss = 3.007e-01, Accuracy = 90.92%\n",
      "000004250: Training Loss = 3.407e-01, Accuracy = 89.60%\n",
      "000004300: Training Loss = 3.106e-01, Accuracy = 90.68%\n",
      "000004350: Training Loss = 3.726e-01, Accuracy = 88.40%\n",
      "000004400: Training Loss = 3.193e-01, Accuracy = 90.16%\n",
      "000004450: Training Loss = 3.433e-01, Accuracy = 90.08%\n",
      "000004500: Training Loss = 3.235e-01, Accuracy = 89.52%\n",
      "000004500: #### 9 Epochs: Val Loss = 2.912e-01, Accuracy = 92.10%\n",
      "000004550: Training Loss = 3.324e-01, Accuracy = 90.16%\n",
      "000004600: Training Loss = 3.168e-01, Accuracy = 90.76%\n",
      "000004650: Training Loss = 2.941e-01, Accuracy = 90.88%\n",
      "000004700: Training Loss = 3.345e-01, Accuracy = 90.16%\n",
      "000004750: Training Loss = 2.713e-01, Accuracy = 91.80%\n",
      "000004800: Training Loss = 2.862e-01, Accuracy = 91.44%\n",
      "000004850: Training Loss = 3.122e-01, Accuracy = 90.80%\n",
      "000004900: Training Loss = 2.871e-01, Accuracy = 91.60%\n",
      "000004950: Training Loss = 3.476e-01, Accuracy = 89.88%\n",
      "000005000: Training Loss = 3.062e-01, Accuracy = 90.92%\n",
      "000005000: #### 10 Epochs: Val Loss = 2.878e-01, Accuracy = 92.40%\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "from os.path import normpath as fn\n",
    "from time import time\n",
    "\n",
    "# Load data\n",
    "data = np.load(fn('inputs/mnist_26k.npz'))\n",
    "\n",
    "train_im = np.float32(data['im_train'])/255.-0.5\n",
    "train_im = np.reshape(train_im,[-1,28,28,1])\n",
    "train_lb = data['lbl_train']\n",
    "\n",
    "val_im = np.float32(data['im_val'])/255.-0.5\n",
    "val_im = np.reshape(val_im,[-1,28,28,1])\n",
    "val_lb = data['lbl_val']\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "# Inputs and parameters\n",
    "inp = edf.Value()\n",
    "lab = edf.Value()\n",
    "\n",
    "K1 = edf.Param()\n",
    "B1 = edf.Param()\n",
    "\n",
    "K2 = edf.Param()\n",
    "B2 = edf.Param()\n",
    "\n",
    "W3 = edf.Param()\n",
    "B3 = edf.Param()\n",
    "\n",
    "\n",
    "# Model\n",
    "y = edf.conv2(inp,K1)\n",
    "# y = edf.down2(y);\n",
    "y = edf.avg_pool_with_stride(y, 2, 2) # replace downsampling with avg pooling with stride\n",
    "y = edf.add(y,B1)\n",
    "y = edf.RELU(y)\n",
    "\n",
    "y = edf.conv2(y,K2)\n",
    "# y = edf.down2(y);\n",
    "y = edf.avg_pool_with_stride(y, 2, 2) # replace downsampling with avg pooling with stride\n",
    "y = edf.add(y,B2)\n",
    "y = edf.RELU(y)\n",
    "\n",
    "\n",
    "y = edf.flatten(y)\n",
    "\n",
    "y = edf.matmul(y,W3)\n",
    "y = edf.add(y,B3) # This is our final prediction\n",
    "\n",
    "\n",
    "# Cross Entropy of Soft-max\n",
    "loss = edf.smaxloss(y,lab)\n",
    "loss = edf.mean(loss)\n",
    "\n",
    "# Accuracy\n",
    "acc = edf.accuracy(y,lab)\n",
    "acc = edf.mean(acc)\n",
    "\n",
    "###################################\n",
    "\n",
    "# Init Weights\n",
    "def xavier(shape):\n",
    "    sq = np.sqrt(3.0/np.prod(shape[:-1]))\n",
    "    return np.random.uniform(-sq,sq,shape)\n",
    "\n",
    "C1 = 8\n",
    "C2 = 16\n",
    "\n",
    "K1.set(xavier((4,4,1,C1)))\n",
    "B1.set(np.zeros((C1)))\n",
    "\n",
    "K2.set(xavier((2,2,C1,C2)))\n",
    "B2.set(np.zeros((C2)))\n",
    "\n",
    "W3.set(xavier((C2*25,10)))\n",
    "B3.set(np.zeros((10)))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "BSZ=50\n",
    "lr=0.001\n",
    "\n",
    "NUM_EPOCH=10\n",
    "DISPITER=50\n",
    "batches = range(0,len(train_lb)-BSZ+1,BSZ)\n",
    "\n",
    "## Implement Momentum and uncomment following line\n",
    "edf.init_momentum()\n",
    "\n",
    "\n",
    "niter=0; avg_loss = 0.; avg_acc = 0.\n",
    "for ep in range(NUM_EPOCH+1):\n",
    "\n",
    "    # As we train, let's keep track of val accuracy\n",
    "    vacc = 0.; vloss = 0.; viter = 0\n",
    "    for b in range(0,len(val_lb)-BSZ+1,BSZ):\n",
    "        inp.set(val_im[b:b+BSZ,...]); lab.set(val_lb[b:b+BSZ])\n",
    "        edf.Forward()\n",
    "        viter = viter + 1;vacc = vacc + acc.top;vloss = vloss + loss.top\n",
    "    vloss = vloss / viter; vacc = vacc / viter * 100\n",
    "    print(\"%09d: #### %d Epochs: Val Loss = %.3e, Accuracy = %.2f%%\" % (niter,ep,vloss,vacc))\n",
    "    if ep == NUM_EPOCH:\n",
    "        break\n",
    "\n",
    "    # Shuffle Training Set\n",
    "    idx = np.random.permutation(len(train_lb))\n",
    "\n",
    "    # Train one epoch\n",
    "    for b in batches:\n",
    "        # Load a batch\n",
    "        inp.set(train_im[idx[b:b+BSZ],...])\n",
    "        lab.set(train_lb[idx[b:b+BSZ]])\n",
    "\n",
    "        edf.Forward()\n",
    "        avg_loss = avg_loss + loss.top; avg_acc = avg_acc + acc.top;\n",
    "        niter = niter + 1\n",
    "        if niter % DISPITER == 0:\n",
    "            avg_loss = avg_loss / DISPITER; avg_acc = avg_acc / DISPITER * 100\n",
    "            print(\"%09d: Training Loss = %.3e, Accuracy = %.2f%%\" % (niter,avg_loss,avg_acc))\n",
    "            avg_loss = 0.; avg_acc = 0.;\n",
    "\n",
    "        edf.Backward(loss)\n",
    "        #edf.SGD(lr)\n",
    "        # Replace previous line with following\n",
    "        edf.momentum(lr,0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
